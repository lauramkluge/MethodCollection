{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Gradient Boosting",
   "id": "ea0ed127a7c269b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "While in RandomForest a lot of trees are built independently of each other in gradient boosting trees are built sequentially.\n",
    "\n",
    "**Algorithm**\n",
    "\n",
    "Input: Loss function $L$, Data $(x_i, y_i), i=1...N$, number of estimators built $M$\n",
    "\n",
    "**1. Initialize**\n",
    "\n",
    "$f_0(x)= argmin_\\gamma \\sum_{i=1}^N L(y_i, \\gamma)$       # this means that the first estimator estimates the same constant $\\gamma$ for all data points\n",
    "\n",
    "**2. For m=1 to $M$:**\n",
    "\n",
    "a) For $i=1,..N$ compute $r_{im}=-\\left[\\frac {\\partial L(y_i, f(x_i))}{\\partial f(x_i)}\\right]_{f=f_{m-1}}$        # \"in which direction do we need to improve to minimize the loss in x_i\"\n",
    "\n",
    "b) Fit a regression tree to $(x_i, r_{im})$ with regions $R_{jm}$ $j=1, J_m$, # Capture the improvement directions for all data-points in a regression tree\n",
    "\n",
    "c) For $j=1,..., J_m$ compute the improvement $\\gamma_{jm}= argmin_\\gamma \\sum_{x_i\\in R_{jm}} L(y_i, f_{m-1}(x_i)+\\gamma)$  # for each region of the 'improvement regression tree' fit a constant improvement\n",
    "\n",
    "d) Update $f_m (x) = f_{m-1}(x)+\\sum_{j=1}^{J_m} \\gamma_{jm} 1(x\\in R_{jm})$\n",
    "\n",
    "**3. Output** $f_M$"
   ],
   "id": "35e09840f09f814d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In the following we will work with XGBoost which is an implementation of gradient boosting with additional features focused on performance and speed.",
   "id": "530cad8503c06377"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Coding",
   "id": "eb7757f8a5746aec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Setup:",
   "id": "39d362dca9caa84e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T12:02:08.481412Z",
     "start_time": "2025-08-01T12:02:08.439029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv('melb_data.csv')\n",
    "\n",
    "# Select subset of predictors\n",
    "cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\n",
    "X = data[cols_to_use]\n",
    "\n",
    "# Select target\n",
    "y = data.Price\n",
    "\n",
    "# Separate data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y)"
   ],
   "id": "6df775226a933b1f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T12:02:10.066648Z",
     "start_time": "2025-08-01T12:02:09.930729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "my_model = XGBRegressor()\n",
    "my_model.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "predictions = my_model.predict(X_valid)\n",
    "print(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_valid)))"
   ],
   "id": "12ca03e50366e4c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 240970.94236238956\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Parameter Tuning of XGBoost\n",
    "\n",
    "https://xgboost.readthedocs.io/en/stable/python/python_api.html#module-xgboost.sklearn"
   ],
   "id": "8b1b68dd7797445f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`n_estimators`\n",
    "n_estimators specifies how many times to go through the modeling cycle described above. It is equal to the number of models that we include in the ensemble.\n",
    "\n",
    "\n",
    "`early_stopping_rounds¶`\n",
    "early_stopping_rounds offers a way to automatically find the ideal value for n_estimators. Early stopping causes the model to stop iterating when the validation score stops improving, even if we aren't at the hard stop for n_estimators. It's smart to set a high value for n_estimators and then use early_stopping_rounds to find the optimal time to stop iterating.\n",
    "Since random chance sometimes causes a single round where validation scores don't improve, you need to specify a number for how many rounds of straight deterioration to allow before stopping. Setting early_stopping_rounds=5 is a reasonable choice. I\n",
    "\n",
    "\n",
    "`learning_rate¶`\n",
    "Instead of getting predictions by simply adding up the predictions from each component model, we can multiply the predictions from each model by a small number (known as the learning rate) before adding them in. This means each tree we add to the ensemble helps us less. So, we can set a higher value for n_estimators without overfitting. If we use early stopping, the appropriate number of trees will be determined automatically. In general, a small learning rate and large number of estimators will yield more accurate XGBoost models, though it will also take the model longer to train since it does more iterations through the cycle.\n",
    "\n",
    "\n",
    "`n_jobs¶`\n",
    "On larger datasets where runtime is a consideration, you can use parallelism to build your models faster. It's common to set the parameter n_jobs equal to the number of cores on your machine. On smaller datasets, this won't help. The resulting model won't be any better, so micro-optimizing for fitting time is typically nothing but a distraction. But, it's useful in large datasets where you would otherwise spend a long time waiting during the fit command."
   ],
   "id": "1f2637163bfaf9de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T12:32:42.539314Z",
     "start_time": "2025-08-01T12:32:42.418293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "my_model = XGBRegressor(n_estimators=1000, early_stopping_rounds=5,learning_rate=0.2, n_jobs=4)\n",
    "my_model.fit(X_train, y_train,\n",
    "             eval_set=[(X_valid, y_valid)],\n",
    "             verbose=False)                     # If verbose is True and an evaluation set is used, the evaluation metric measured on the validation set is printed to stdout at each boosting stage.\n",
    "predictions = my_model.predict(X_valid)\n",
    "print(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_valid)))"
   ],
   "id": "6b12e751713abb0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 240812.90228276877\n"
     ]
    }
   ],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
