{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Initialisierung",
   "id": "cd60e645659d0f14"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-30T20:16:41.019594Z",
     "start_time": "2025-07-30T20:16:39.612051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv('melb_data.csv')\n",
    "\n",
    "# Separate target from predictors\n",
    "y = data.Price\n",
    "X = data.drop(['Price'], axis=1)\n",
    "\n",
    "# Divide data into training and validation subsets\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)\n",
    "\n",
    "# Drop columns with missing values (simplest approach)\n",
    "cols_with_missing = [col for col in X_train_full.columns if X_train_full[col].isnull().any()]\n",
    "X_train_full.drop(cols_with_missing, axis=1, inplace=True)\n",
    "X_valid_full.drop(cols_with_missing, axis=1, inplace=True)\n",
    "\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and\n",
    "                        X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = low_cardinality_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()\n",
    "X_train.head()"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      Type Method             Regionname  Rooms  Distance  Postcode  Bedroom2  \\\n",
       "12167    u      S  Southern Metropolitan      1       5.0    3182.0       1.0   \n",
       "6524     h     SA   Western Metropolitan      2       8.0    3016.0       2.0   \n",
       "8413     h      S   Western Metropolitan      3      12.6    3020.0       3.0   \n",
       "2919     u     SP  Northern Metropolitan      3      13.0    3046.0       3.0   \n",
       "6043     h      S   Western Metropolitan      3      13.3    3020.0       3.0   \n",
       "\n",
       "       Bathroom  Landsize  Lattitude  Longtitude  Propertycount  \n",
       "12167       1.0       0.0  -37.85984    144.9867        13240.0  \n",
       "6524        2.0     193.0  -37.85800    144.9005         6380.0  \n",
       "8413        1.0     555.0  -37.79880    144.8220         3755.0  \n",
       "2919        1.0     265.0  -37.70830    144.9158         8870.0  \n",
       "6043        1.0     673.0  -37.76230    144.8272         4217.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Method</th>\n",
       "      <th>Regionname</th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Postcode</th>\n",
       "      <th>Bedroom2</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Landsize</th>\n",
       "      <th>Lattitude</th>\n",
       "      <th>Longtitude</th>\n",
       "      <th>Propertycount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12167</th>\n",
       "      <td>u</td>\n",
       "      <td>S</td>\n",
       "      <td>Southern Metropolitan</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3182.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-37.85984</td>\n",
       "      <td>144.9867</td>\n",
       "      <td>13240.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6524</th>\n",
       "      <td>h</td>\n",
       "      <td>SA</td>\n",
       "      <td>Western Metropolitan</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3016.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>-37.85800</td>\n",
       "      <td>144.9005</td>\n",
       "      <td>6380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8413</th>\n",
       "      <td>h</td>\n",
       "      <td>S</td>\n",
       "      <td>Western Metropolitan</td>\n",
       "      <td>3</td>\n",
       "      <td>12.6</td>\n",
       "      <td>3020.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>555.0</td>\n",
       "      <td>-37.79880</td>\n",
       "      <td>144.8220</td>\n",
       "      <td>3755.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2919</th>\n",
       "      <td>u</td>\n",
       "      <td>SP</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "      <td>3</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3046.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>-37.70830</td>\n",
       "      <td>144.9158</td>\n",
       "      <td>8870.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6043</th>\n",
       "      <td>h</td>\n",
       "      <td>S</td>\n",
       "      <td>Western Metropolitan</td>\n",
       "      <td>3</td>\n",
       "      <td>13.3</td>\n",
       "      <td>3020.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>673.0</td>\n",
       "      <td>-37.76230</td>\n",
       "      <td>144.8272</td>\n",
       "      <td>4217.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T20:16:41.212987Z",
     "start_time": "2025-07-30T20:16:41.073901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Function for comparing different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ],
   "id": "13ca8bcab06f2fe3",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Ways to deal with categorical variables",
   "id": "43d908a5f957d7c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First obtain categorical variables:",
   "id": "eea427a4e7e97091"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T20:16:41.284115Z",
     "start_time": "2025-07-30T20:16:41.278707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get list of categorical variables\n",
    "s = (X_train.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print(\"Categorical variables:\")\n",
    "print(object_cols)"
   ],
   "id": "8c7e304bcb087603",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical variables:\n",
      "['Type', 'Method', 'Regionname']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Approach 1: Delete Categoricals",
   "id": "7178dbfadd624810"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T20:16:45.647968Z",
     "start_time": "2025-07-30T20:16:41.447922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
    "\n",
    "print(\"MAE from Approach 1 (Drop categorical variables):\")\n",
    "print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))"
   ],
   "id": "120223c4bb78b45",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 1 (Drop categorical variables):\n",
      "175703.48185157913\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Approach 2A: Ordinal encoding",
   "id": "c7c1320ebab8ebcf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ordinal encoding assigns each unique value to a different integer. This approach assumes an ordering of the categories, but not all categorical variables have a clear ordering in the values. If there exists a clear ordering we refer to those variables as **_ordinal variables_**. For tree-based models (like decision trees and random forests), you can expect ordinal encoding to work well with ordinal variables.",
   "id": "7b21ec8c7d51969d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T20:16:50.350755Z",
     "start_time": "2025-07-30T20:16:45.681765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Make copy to avoid changing original data\n",
    "label_X_train = X_train.copy()\n",
    "label_X_valid = X_valid.copy()\n",
    "\n",
    "# Apply ordinal encoder to each column with categorical data\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])\n",
    "label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])\n",
    "# For each column, we randomly assign each unique value to a different integer. This is a common approach that is simpler than providing custom labels; however, we can expect an additional boost in performance if we provide better-informed labels for all ordinal variables.\n",
    "\n",
    "print(\"MAE from Approach 2 (Ordinal Encoding):\")\n",
    "print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))"
   ],
   "id": "b15cf298f9e7f15e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 2 (Ordinal Encoding):\n",
      "165936.40548390493\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T20:16:50.421957Z",
     "start_time": "2025-07-30T20:16:50.414972Z"
    }
   },
   "cell_type": "code",
   "source": "label_X_train['Regionname'].head()",
   "id": "68283e141215fcd4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12167    5.0\n",
       "6524     6.0\n",
       "8413     6.0\n",
       "2919     2.0\n",
       "6043     6.0\n",
       "Name: Regionname, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### _Problem: Labels in Validation are no subset of Labels in Training_\n",
    "\n",
    "Fitting an ordinal encoder to a column in the training data creates a corresponding integer-valued label for each unique value that appears in the training data. In the case that the validation data contains values that don't also appear in the training data, the encoder will throw an error, because these values won't have an integer assigned to them.\n",
    "\n",
    "This is a common problem that and there are many approaches to fixing this issue. For instance, you can write a custom ordinal encoder to deal with new categories. The simplest approach, however, is to drop the problematic categorical columns.\n",
    "\n",
    "The code in the cell below saves the problematic columns to a Python list bad_label_cols. Likewise, columns that can be safely ordinal encoded are stored in good_label_cols"
   ],
   "id": "a154f87e38938e17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T20:18:46.427677Z",
     "start_time": "2025-07-30T20:18:46.412886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Categorical columns in the training data\n",
    "object_cols = [col for col in X_train_full.columns if X_train_full[col].dtype == \"object\"]\n",
    "\n",
    "# Columns that can be safely ordinal encoded\n",
    "good_label_cols = [col for col in object_cols if set(X_valid_full[col]).issubset(set(X_train_full[col]))]\n",
    "\n",
    "# Problematic columns that will be dropped from the dataset\n",
    "bad_label_cols = list(set(object_cols) - set(good_label_cols))\n",
    "\n",
    "print('Categorical columns that will be ordinal encoded:', good_label_cols)\n",
    "print('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)\n",
    "\n",
    "# Drop categorical columns that will not be encoded\n",
    "label_X_train2 = X_train_full.drop(bad_label_cols, axis=1)\n",
    "label_X_valid2 = X_valid_full.drop(bad_label_cols, axis=1)"
   ],
   "id": "47d897934f60afaa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns that will be ordinal encoded: ['Type', 'Method', 'Date', 'Regionname']\n",
      "\n",
      "Categorical columns that will be dropped from the dataset: ['Address', 'Suburb', 'SellerG']\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T20:21:10.672350Z",
     "start_time": "2025-07-30T20:21:05.922067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply ordinal encoder\n",
    "ordinal_encoder= OrdinalEncoder()\n",
    "label_X_train2[good_label_cols] = ordinal_encoder.fit_transform(X_train_full[good_label_cols])\n",
    "label_X_valid2[good_label_cols] = ordinal_encoder.transform(X_valid_full[good_label_cols])\n",
    "\n",
    "print(\"MAE from Approach 2b (Ordinal Encoding, Extended):\")\n",
    "print(score_dataset(label_X_train2, label_X_valid2, y_train, y_valid))"
   ],
   "id": "d921e16d439781e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 2b (Ordinal Encoding, Extended):\n",
      "167305.75850147277\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Approach 3: One-Hot Encoding",
   "id": "76bfdd461e2dea9d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In contrast to ordinal encoding, one-hot encoding does not assume an ordering of the categories. Thus, you can expect this approach to work particularly well if there is no clear ordering in the categorical data (e.g., \"Red\" is neither more nor less than \"Yellow\"). We refer to **_categorical variables_** without an intrinsic ranking as nominal variables.\n",
    "\n",
    "One-hot encoding generally does not perform well if the categorical variable takes on a large number of values (i.e., you generally won't use it for variables taking more than 15 different values)."
   ],
   "id": "23dca349cd0c811a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T20:16:51.911912300Z",
     "start_time": "2025-07-30T19:51:48.234265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False) # 'ignore': unknown categories will be set to zero in transform\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "\n",
    "# Ensure all columns have string type\n",
    "OH_X_train.columns = OH_X_train.columns.astype(str)\n",
    "OH_X_valid.columns = OH_X_valid.columns.astype(str)\n",
    "\n",
    "print(\"MAE from Approach 3 (One-Hot Encoding):\")\n",
    "print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))"
   ],
   "id": "2798beaecfd55a3a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 3 (One-Hot Encoding):\n",
      "166089.4893009678\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For large datasets with many rows, one-hot encoding can greatly expand the size of the dataset. For this reason, we typically will only one-hot encode columns with relatively low cardinality. Then, high cardinality columns can either be dropped from the dataset (as in done in Initialisierung with X_train, X_valid), or we can use ordinal encoding.",
   "id": "4a027122c9a376f8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Investigate cardinality (number of unique values in each column) of the full data set:",
   "id": "545102d13dfa0e90"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T20:23:00.838764Z",
     "start_time": "2025-07-30T20:23:00.827979Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Type', 3),\n",
       " ('Method', 5),\n",
       " ('Regionname', 8),\n",
       " ('Date', 58),\n",
       " ('SellerG', 251),\n",
       " ('Suburb', 308),\n",
       " ('Address', 10742)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18,
   "source": [
    "# Get number of unique entries in each column with categorical data\n",
    "object_nunique = list(map(lambda col: X_train_full[col].nunique(), object_cols))\n",
    "d = dict(zip(object_cols, object_nunique))\n",
    "\n",
    "# Print number of unique entries by column, in ascending order\n",
    "sorted(d.items(), key=lambda x: x[1])"
   ],
   "id": "1e79ecf93b94e6d1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
